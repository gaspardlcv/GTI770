{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adrien/anaconda3/envs/GPI770/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/adrien/anaconda3/envs/GPI770/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/adrien/anaconda3/envs/GPI770/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/adrien/anaconda3/envs/GPI770/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/adrien/anaconda3/envs/GPI770/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/adrien/anaconda3/envs/GPI770/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import csv\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import cv2\n",
    "import pandas as pd\n",
    "from sklearn import svm\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer, accuracy_score, f1_score\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import tensorboard\n",
    "from keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv(url):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        url (string): the url of the file\n",
    "    Returns:\n",
    "        df: the dataframe filled\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(url, header=None)\n",
    "    df.head()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Réseaux neuronaux avec TensorFlow \n",
    "Multi-Layer Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Hyperparametre | valeur de base   |\n",
    "|------|------|\n",
    "|   Nombre de couches total | 4, y compris la couche d’entrée (input) et la couche de sortie (output) |\n",
    "|   Nombre de perceptrons dans la couche cachée (hidden layer) | 100, 100, 2 |\n",
    "|   Nombre d’itérations (epochs) | 60 |\n",
    "|   Taux d’apprentissage (learningrate) | 0.0005 |\n",
    "|   Batch size | 100 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_layers = 4\n",
    "nb_perceptron = [100,100,2]\n",
    "nb_iteration = 60\n",
    "learning_rate = 0.0005\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version longue\n",
    "1. Faire for loop pour Avec la librairie TensorFlow (avec l’API tf.keras) \n",
    "    1. Vous pouvez vous inspirer de la littérature, de recherches Internet, de la documentation de TensorFlow ou d’autres sources\n",
    "    2. Ce modèle doit être original\n",
    "    3. Il s’agit ici de construire un Multi-Layer Perceptron model.\n",
    "    4. Faites attention aux nombres de perceptrons à l’entrée et à la sortie; la première valeur doit concorder avec le  nombre de caractéristiques [features] que votre vecteur en entrée comporte [son nombre de dimensions] alors que la deuxième doit concorder avec le nombre de classes que vous avez à la sortie.\n",
    "        \n",
    "2. Dans votre code source, vous devrez ajouter le code nécessaire à l’outil TensorBoard (TensorFlow) Cet outil vous permettra alors de suivre l’évolution des hyperparamètres, via un serveur Web local sur votre machine, et vous facilitera grandement  la tâche dans votre étude des réseaux neuronaux. Vous devez ajouter le code permettant de capturer les valeurs de la précision [accuracy] et la mesure de l’inconsistance entre la valeur prédite et la vraie valeur [loss].\n",
    "3. Voir tableau Hyperparametre, vous devez sélectionner trois valeurs différentes et lancer l’apprentissage avec ces valeurs. **Vous aurez alors 12 modèles d’apprentissages différents**. Notez les résultats de la précision [accuracy] que vous avez obtenue au final. À la fin de vos manipulations, vous devriez avoir, par le biais de TensorBoard, des graphiques comme ci-dessous décrivant la précision [accuracy] et le loss. \n",
    "    1. *Aller voir enoncé pour exemples de graph*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version courte\n",
    "1. Faire une for loop qui joue avec les hyperparametres\n",
    "    1. Creer un reseau de neuronne avec les hyperparametres\n",
    "    2. Utiliser TensorBoard pour capturer les valeurs de précision, de loss, le score F1 de chaque reseau\n",
    "    3. Prenez en note le temps d’exécution de vos modèles autant en phase d’apprentissage qu’en phase de test.\n",
    "    4. Afficher les graphiques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OneHot(x):\n",
    "    if x == 1:\n",
    "        res = [1,0]\n",
    "    elif x == 0:\n",
    "        res = [0,0]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the csv and get the dataframe\n",
    "df = read_csv(\"../galaxy_feature_vectors.csv\")\n",
    "\n",
    "# replace the output by a binairy vector \n",
    "new_labels = []\n",
    "for i in df[75]:\n",
    "    i = OneHot(i)\n",
    "    new_labels.append(i)\n",
    "\n",
    "# get the train and test sample\n",
    "df = df.drop(0, axis=1)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(df.loc[:, df.columns != 75], new_labels, test_size=0.2,stratify=df[75])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set labels as DataFrame\n",
    "Y_train = pd.DataFrame(Y_train)\n",
    "Y_test = pd.DataFrame(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the Train set into train and validate set\n",
    "nx_train, nx_val, ny_train, ny_val = train_test_split(X_train, Y_train, test_size=0.2,stratify=Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the model \n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/adrien/anaconda3/envs/GPI770/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "#First hidden layer with 100 neurons \n",
    "model.add(Dense(units=100, activation='sigmoid', input_dim = 74))\n",
    "\n",
    "#Second hidden layer with 100 neurons \n",
    "model.add(Dense(units=100, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Last layer, the activation layer with 2 outputs\n",
    "model.add(Dense(units = 2, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compile the model\n",
    "sgd = SGD(lr=learning_rate)\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=sgd,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/adrien/anaconda3/envs/GPI770/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 10820 samples, validate on 2706 samples\n",
      "Epoch 1/60\n",
      "10820/10820 [==============================] - 0s 28us/step - loss: 0.5165 - accuracy: 0.7405 - val_loss: 0.5049 - val_accuracy: 0.7404\n",
      "Epoch 2/60\n",
      "10820/10820 [==============================] - 0s 24us/step - loss: 0.4904 - accuracy: 0.7405 - val_loss: 0.4770 - val_accuracy: 0.7404\n",
      "Epoch 3/60\n",
      "10820/10820 [==============================] - 0s 21us/step - loss: 0.4664 - accuracy: 0.7405 - val_loss: 0.4565 - val_accuracy: 0.7406\n",
      "Epoch 4/60\n",
      "10820/10820 [==============================] - 0s 21us/step - loss: 0.4486 - accuracy: 0.7405 - val_loss: 0.4411 - val_accuracy: 0.7406\n",
      "Epoch 5/60\n",
      "10820/10820 [==============================] - 0s 20us/step - loss: 0.4350 - accuracy: 0.7405 - val_loss: 0.4292 - val_accuracy: 0.7406\n",
      "Epoch 6/60\n",
      "10820/10820 [==============================] - 0s 20us/step - loss: 0.4244 - accuracy: 0.7405 - val_loss: 0.4198 - val_accuracy: 0.7406\n",
      "Epoch 7/60\n",
      "10820/10820 [==============================] - 0s 23us/step - loss: 0.4159 - accuracy: 0.7562 - val_loss: 0.4122 - val_accuracy: 0.7596\n",
      "Epoch 8/60\n",
      "10820/10820 [==============================] - 0s 22us/step - loss: 0.4090 - accuracy: 0.7595 - val_loss: 0.4059 - val_accuracy: 0.7596\n",
      "Epoch 9/60\n",
      "10820/10820 [==============================] - 0s 21us/step - loss: 0.4033 - accuracy: 0.7595 - val_loss: 0.4007 - val_accuracy: 0.7596\n",
      "Epoch 10/60\n",
      "10820/10820 [==============================] - 0s 20us/step - loss: 0.3984 - accuracy: 0.7595 - val_loss: 0.3962 - val_accuracy: 0.7596\n",
      "Epoch 11/60\n",
      "10820/10820 [==============================] - 0s 21us/step - loss: 0.3943 - accuracy: 0.7595 - val_loss: 0.3924 - val_accuracy: 0.7596\n",
      "Epoch 12/60\n",
      "10820/10820 [==============================] - 0s 21us/step - loss: 0.3908 - accuracy: 0.7595 - val_loss: 0.3892 - val_accuracy: 0.7596\n",
      "Epoch 13/60\n",
      "10820/10820 [==============================] - 0s 21us/step - loss: 0.3877 - accuracy: 0.7595 - val_loss: 0.3863 - val_accuracy: 0.7596\n",
      "Epoch 14/60\n",
      "10820/10820 [==============================] - 0s 22us/step - loss: 0.3851 - accuracy: 0.7595 - val_loss: 0.3838 - val_accuracy: 0.7596\n",
      "Epoch 15/60\n",
      "10820/10820 [==============================] - 0s 22us/step - loss: 0.3827 - accuracy: 0.7595 - val_loss: 0.3815 - val_accuracy: 0.7596\n",
      "Epoch 16/60\n",
      "10820/10820 [==============================] - 0s 28us/step - loss: 0.3806 - accuracy: 0.7595 - val_loss: 0.3796 - val_accuracy: 0.7596\n",
      "Epoch 17/60\n",
      "10820/10820 [==============================] - 0s 22us/step - loss: 0.3787 - accuracy: 0.7595 - val_loss: 0.3778 - val_accuracy: 0.7596\n",
      "Epoch 18/60\n",
      "10820/10820 [==============================] - 0s 30us/step - loss: 0.3770 - accuracy: 0.7595 - val_loss: 0.3762 - val_accuracy: 0.7596\n",
      "Epoch 19/60\n",
      "10820/10820 [==============================] - 0s 21us/step - loss: 0.3754 - accuracy: 0.7595 - val_loss: 0.3747 - val_accuracy: 0.7596\n",
      "Epoch 20/60\n",
      "10820/10820 [==============================] - 0s 21us/step - loss: 0.3740 - accuracy: 0.7595 - val_loss: 0.3734 - val_accuracy: 0.7596\n",
      "Epoch 21/60\n",
      "10820/10820 [==============================] - 0s 21us/step - loss: 0.3728 - accuracy: 0.7595 - val_loss: 0.3721 - val_accuracy: 0.7596\n",
      "Epoch 22/60\n",
      "10820/10820 [==============================] - 0s 21us/step - loss: 0.3716 - accuracy: 0.7595 - val_loss: 0.3710 - val_accuracy: 0.7596\n",
      "Epoch 23/60\n",
      "10820/10820 [==============================] - 0s 27us/step - loss: 0.3705 - accuracy: 0.7595 - val_loss: 0.3700 - val_accuracy: 0.7596\n",
      "Epoch 24/60\n",
      "10820/10820 [==============================] - 0s 23us/step - loss: 0.3695 - accuracy: 0.7595 - val_loss: 0.3691 - val_accuracy: 0.7596\n",
      "Epoch 25/60\n",
      "10820/10820 [==============================] - 0s 20us/step - loss: 0.3686 - accuracy: 0.7595 - val_loss: 0.3682 - val_accuracy: 0.7596\n",
      "Epoch 26/60\n",
      "10820/10820 [==============================] - 0s 21us/step - loss: 0.3678 - accuracy: 0.7595 - val_loss: 0.3674 - val_accuracy: 0.7596\n",
      "Epoch 27/60\n",
      "10820/10820 [==============================] - 0s 20us/step - loss: 0.3670 - accuracy: 0.7595 - val_loss: 0.3666 - val_accuracy: 0.7596\n",
      "Epoch 28/60\n",
      "10820/10820 [==============================] - 0s 21us/step - loss: 0.3663 - accuracy: 0.7595 - val_loss: 0.3659 - val_accuracy: 0.7596\n",
      "Epoch 29/60\n",
      "10820/10820 [==============================] - 0s 33us/step - loss: 0.3656 - accuracy: 0.7595 - val_loss: 0.3652 - val_accuracy: 0.7596\n",
      "Epoch 30/60\n",
      "10820/10820 [==============================] - 0s 21us/step - loss: 0.3649 - accuracy: 0.7595 - val_loss: 0.3646 - val_accuracy: 0.7596\n",
      "Epoch 31/60\n",
      "10820/10820 [==============================] - 0s 22us/step - loss: 0.3643 - accuracy: 0.7595 - val_loss: 0.3640 - val_accuracy: 0.7596\n",
      "Epoch 32/60\n",
      "10820/10820 [==============================] - 0s 22us/step - loss: 0.3638 - accuracy: 0.7595 - val_loss: 0.3635 - val_accuracy: 0.7596\n",
      "Epoch 33/60\n",
      "10820/10820 [==============================] - 0s 21us/step - loss: 0.3632 - accuracy: 0.7595 - val_loss: 0.3630 - val_accuracy: 0.7596\n",
      "Epoch 34/60\n",
      "10820/10820 [==============================] - 0s 22us/step - loss: 0.3627 - accuracy: 0.7595 - val_loss: 0.3625 - val_accuracy: 0.7596\n",
      "Epoch 35/60\n",
      "10820/10820 [==============================] - 0s 40us/step - loss: 0.3623 - accuracy: 0.7595 - val_loss: 0.3620 - val_accuracy: 0.7596\n",
      "Epoch 36/60\n",
      "10820/10820 [==============================] - 0s 20us/step - loss: 0.3618 - accuracy: 0.7595 - val_loss: 0.3616 - val_accuracy: 0.7596\n",
      "Epoch 37/60\n",
      "10820/10820 [==============================] - 0s 36us/step - loss: 0.3614 - accuracy: 0.7595 - val_loss: 0.3612 - val_accuracy: 0.7596\n",
      "Epoch 38/60\n",
      "10820/10820 [==============================] - 0s 23us/step - loss: 0.3610 - accuracy: 0.7595 - val_loss: 0.3608 - val_accuracy: 0.7596\n",
      "Epoch 39/60\n",
      "10820/10820 [==============================] - 0s 29us/step - loss: 0.3606 - accuracy: 0.7595 - val_loss: 0.3604 - val_accuracy: 0.7596\n",
      "Epoch 40/60\n",
      "10820/10820 [==============================] - 0s 37us/step - loss: 0.3603 - accuracy: 0.7595 - val_loss: 0.3601 - val_accuracy: 0.7596\n",
      "Epoch 41/60\n",
      "10820/10820 [==============================] - 0s 35us/step - loss: 0.3599 - accuracy: 0.7595 - val_loss: 0.3597 - val_accuracy: 0.7596\n",
      "Epoch 42/60\n",
      "10820/10820 [==============================] - 0s 20us/step - loss: 0.3596 - accuracy: 0.7595 - val_loss: 0.3594 - val_accuracy: 0.7596\n",
      "Epoch 43/60\n",
      "10820/10820 [==============================] - 0s 20us/step - loss: 0.3593 - accuracy: 0.7595 - val_loss: 0.3591 - val_accuracy: 0.7596\n",
      "Epoch 44/60\n",
      "10820/10820 [==============================] - 0s 19us/step - loss: 0.3590 - accuracy: 0.7595 - val_loss: 0.3588 - val_accuracy: 0.7596\n",
      "Epoch 45/60\n",
      "10820/10820 [==============================] - 0s 23us/step - loss: 0.3587 - accuracy: 0.7595 - val_loss: 0.3585 - val_accuracy: 0.7596\n",
      "Epoch 46/60\n",
      "10820/10820 [==============================] - 0s 21us/step - loss: 0.3584 - accuracy: 0.7595 - val_loss: 0.3582 - val_accuracy: 0.7596\n",
      "Epoch 47/60\n",
      "10820/10820 [==============================] - 0s 21us/step - loss: 0.3581 - accuracy: 0.7595 - val_loss: 0.3580 - val_accuracy: 0.7596\n",
      "Epoch 48/60\n",
      "10820/10820 [==============================] - 0s 20us/step - loss: 0.3579 - accuracy: 0.7595 - val_loss: 0.3577 - val_accuracy: 0.7596\n",
      "Epoch 49/60\n",
      "10820/10820 [==============================] - 0s 21us/step - loss: 0.3576 - accuracy: 0.7595 - val_loss: 0.3575 - val_accuracy: 0.7596\n",
      "Epoch 50/60\n",
      "10820/10820 [==============================] - 0s 20us/step - loss: 0.3574 - accuracy: 0.7595 - val_loss: 0.3573 - val_accuracy: 0.7596\n",
      "Epoch 51/60\n",
      "10820/10820 [==============================] - 0s 20us/step - loss: 0.3572 - accuracy: 0.7595 - val_loss: 0.3571 - val_accuracy: 0.7596\n",
      "Epoch 52/60\n",
      "10820/10820 [==============================] - 0s 30us/step - loss: 0.3570 - accuracy: 0.7595 - val_loss: 0.3568 - val_accuracy: 0.7596\n",
      "Epoch 53/60\n",
      "10820/10820 [==============================] - 0s 30us/step - loss: 0.3568 - accuracy: 0.7595 - val_loss: 0.3566 - val_accuracy: 0.7596\n",
      "Epoch 54/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10820/10820 [==============================] - 0s 35us/step - loss: 0.3566 - accuracy: 0.7595 - val_loss: 0.3564 - val_accuracy: 0.7596\n",
      "Epoch 55/60\n",
      "10820/10820 [==============================] - 0s 26us/step - loss: 0.3564 - accuracy: 0.7595 - val_loss: 0.3563 - val_accuracy: 0.7596\n",
      "Epoch 56/60\n",
      "10820/10820 [==============================] - 0s 32us/step - loss: 0.3562 - accuracy: 0.7595 - val_loss: 0.3561 - val_accuracy: 0.7596\n",
      "Epoch 57/60\n",
      "10820/10820 [==============================] - 0s 19us/step - loss: 0.3560 - accuracy: 0.7595 - val_loss: 0.3559 - val_accuracy: 0.7596\n",
      "Epoch 58/60\n",
      "10820/10820 [==============================] - 0s 20us/step - loss: 0.3558 - accuracy: 0.7595 - val_loss: 0.3557 - val_accuracy: 0.7596\n",
      "Epoch 59/60\n",
      "10820/10820 [==============================] - 0s 20us/step - loss: 0.3557 - accuracy: 0.7595 - val_loss: 0.3556 - val_accuracy: 0.7596\n",
      "Epoch 60/60\n",
      "10820/10820 [==============================] - 0s 19us/step - loss: 0.3555 - accuracy: 0.7595 - val_loss: 0.3554 - val_accuracy: 0.7596\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7fbee3ec1198>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#3. Entraîner \n",
    "log_dir  = './logs/nn_64'\n",
    "# On va utiliser Tensorboard pour visualizer le progrès de l'entraînement\n",
    "tb_callback = keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "model.fit(nx_train, ny_train, validation_data=(nx_val, ny_val),\n",
    "          epochs=60, batch_size=100, callbacks=[tb_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3382/3382 [==============================] - 0s 22us/step\n",
      "[0.3554232397740723, 0.7594618797302246]\n"
     ]
    }
   ],
   "source": [
    "#4 Evaluer le modèle\n",
    "score = model.evaluate(X_test, Y_test)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machines à vecteur de support "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version Longue\n",
    "1. À l’aide de la librairie scikit-learn, produisez un code source permettant de classifier les galaxies avec l’aide de l’algorithme SVM.\n",
    "2. Plusieurs modèles calculés à l’aide de SVM avec différentes valeurs d’hyperparamètres sont demandés. Fiez-vous aux matrices ci-dessous afin de les réaliser\n",
    "\n",
    "Tableau 5.2 : Matrice des hyperparemètres - SVM kernel = ‘linear’\n",
    "\n",
    "| Poids des classes / Variable C  | class_weight= {‘balanced’}   | \n",
    "|------|------|\n",
    "|   1E-03 | X | \n",
    "|   1E-01 | X | \n",
    "|   1.0 | X | \n",
    "|   10.0 | X | \n",
    "\n",
    "Tableau 5.3 : Matrice des hyperparemètres - SVM kernel = ‘rbf’\n",
    "\n",
    "| Paramètre ɣ(gamma)  / Variable C  | Ɣ=1E-03 | Ɣ=1E-01 | Ɣ=1.0 | Ɣ=10.0 |\n",
    "|------|------|------|------|------|\n",
    "|   1E-03 | X | X | X | X |\n",
    "|   1E-01 | X | X | X | X |\n",
    "|   1.0 | X | X | X | X |\n",
    "|   10.0 | X | X | X | X |\n",
    "\n",
    "Note sur SVM:\n",
    "3. Vous pouvez utiliser la classe sklearn.model_selection.GridSearchCV afin d’effectuer la recherche des meilleurs hyperparamètres. De ce fait, vous n’aurez pas besoin de changer manuellement les valeurs à chaque exécution. De plus, vous pouvez utiliser la classe sklearn.model_selection. StratifiedShuffleSplit afin de diviser l’ensemble de données en un n d’ensembles afin d’effectuer une validation croisée, le tout de manière automatique et aléatoire. Pour plus d’information, fiez-vous à l’exemple donné à la page RBF SVM Parameters disponible en annexe.\n",
    "\n",
    "4. De plus, vous pouvez augmenter la vitesse d’exécution en parallélisant la recherche des hyperparamètres. En effet, la fonction GridSearchCV possède l’argument n_jobs=X où X est le nombre d’exécutions parallèle désiré (généralement proportionnel au nombre de coeurs x86 de la machine sur lequel s’exécute le code Python). Aussi, assurez-vous que votre appel de méthode spécifie une taille de cache suffisante pour le noyau SVM. Par l’intermédiaire du paramètre cache_size, vous serez en mesure de spécifier la taille de la cache désirée, ce qui peut réduire considérablement le temps de résolution de l’optimisation quadratique du problème. Ainsi, en spécifiant cache_size=2048 dans la déclaration de votre classificateur, vous spécifiez que l’algorithme peut utiliser jusqu’à 2 Go de mémoire vive [RAM] à titre de cache pour le noyau. Attention : votre ordinateur doit avoir suffisamment de mémoire vive. Assurez-vous de ne pas spécifier une valeur trop haute pour ne pas saturer la mémoire vive de votre ordinateur.\n",
    "\n",
    "• Notez bien tous les résultats de la précision [accuracy] et le score F1 de chaque modèle calculé. Vous\n",
    "devrez représenter ces résultats dans un tableau dans votre rapport. Prenez en note le temps d’exécution de vos modèles autant en phase d’apprentissage qu’en phase de test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version courte \n",
    "1. Utiliser sklearn.model_selection.GridSearchCV pour trouver les meilleurs hyperparametres\n",
    "2. sklearn.model_selection\n",
    "3. StratifiedShuffleSplit afin de diviser l’ensemble de données en un n d’ensembles afin d’effectuer une validation croisée\n",
    "4. Pour chaque modele calculé noté:\n",
    "    1. Accuracy\n",
    "    2. score F1\n",
    "    3. Temps d'execution en phase apprentissage et en phase test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = read_csv(r\"C:\\Users\\gaspa\\OneDrive\\Bureau\\COURS\\ING 5\\Semestre 1\\Machine Learning\\GTI770\\labs\\galaxy_feature_vectors.csv\")\n",
    "\n",
    "# get the train and test sample\n",
    "df = df.drop(0, axis=1)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(df.loc[:, df.columns != 75], df[75], test_size=0.2,stratify=df[75])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debut du grid search\n",
      "SVC(C=10, cache_size=6000, class_weight='balanced', coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n"
     ]
    }
   ],
   "source": [
    "def SVM(X_train,Y_train,X_test,Y_test,k,c,weight,g) :\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        X_train (list): the train sample\n",
    "        X_test (list): the test sample\n",
    "        Y_train (list): the train output\n",
    "        Y_test (list): the test output\n",
    "    Returns:\n",
    "        \n",
    "    \"\"\"\n",
    "    acc_scorer = make_scorer(accuracy_score)\n",
    "    f1_scorer = make_scorer(f1_score)\n",
    "    scores = {'F1': f1_scorer, 'Accuracy': acc_scorer}\n",
    "\n",
    "    # scale the data : réduire le execution time\n",
    "    scaling = MinMaxScaler(feature_range=(-1,1)).fit(X_train)\n",
    "    X_train = scaling.transform(X_train)\n",
    "    X_test = scaling.transform(X_test)\n",
    "    print(\"Debut du grid search\")\n",
    "    # implemente le gridsearchcv\n",
    "    parameters = {'class_weight':weight,'cache_size':[6000],'C':[0.001,0.1,1,10],'gamma':[0.001,0.1,1,10],'kernel':('linear', 'rbf')}\n",
    "    #parameters = {'class_weight':weight,'cache_size':[6000],'C':[0.001,0.1,1,10],'kernel':['linear']}\n",
    "    model  = svm.SVC()\n",
    "    clf = GridSearchCV(model, parameters, scoring=scores,refit='Accuracy',n_jobs=2,cv=3,return_train_score=True)\n",
    "    clf.fit(X_train, Y_train)\n",
    "    \n",
    "    # sort le best model pour fit et score dessus\n",
    "    best_model = clf.best_estimator_\n",
    "    print(best_model)\n",
    "    \n",
    "    return clf\n",
    "\n",
    "grid = SVM(X_train,Y_train,X_test,Y_test,'linear',10.0,['balanced'],'auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>param_kernel</th>\n",
       "      <th>param_C</th>\n",
       "      <th>param_gamma</th>\n",
       "      <th>mean_test_F1</th>\n",
       "      <th>std_test_F1</th>\n",
       "      <th>mean_test_Accuracy</th>\n",
       "      <th>std_test_Accuracy</th>\n",
       "      <th>rank_test_Accuracy</th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>linear</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0.943794</td>\n",
       "      <td>0.001819</td>\n",
       "      <td>0.941964</td>\n",
       "      <td>0.001815</td>\n",
       "      <td>5</td>\n",
       "      <td>13.844085</td>\n",
       "      <td>1.157593</td>\n",
       "      <td>1.052269</td>\n",
       "      <td>0.090952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>linear</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0.943794</td>\n",
       "      <td>0.001819</td>\n",
       "      <td>0.941964</td>\n",
       "      <td>0.001815</td>\n",
       "      <td>5</td>\n",
       "      <td>12.021735</td>\n",
       "      <td>1.461690</td>\n",
       "      <td>0.967000</td>\n",
       "      <td>0.114822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>linear</td>\n",
       "      <td>10</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.943794</td>\n",
       "      <td>0.001819</td>\n",
       "      <td>0.941964</td>\n",
       "      <td>0.001815</td>\n",
       "      <td>5</td>\n",
       "      <td>11.294003</td>\n",
       "      <td>0.528975</td>\n",
       "      <td>0.904012</td>\n",
       "      <td>0.009436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>linear</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.943794</td>\n",
       "      <td>0.001819</td>\n",
       "      <td>0.941964</td>\n",
       "      <td>0.001815</td>\n",
       "      <td>5</td>\n",
       "      <td>11.250313</td>\n",
       "      <td>1.156103</td>\n",
       "      <td>0.953658</td>\n",
       "      <td>0.102751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>linear</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0.941158</td>\n",
       "      <td>0.002491</td>\n",
       "      <td>0.939302</td>\n",
       "      <td>0.002534</td>\n",
       "      <td>9</td>\n",
       "      <td>5.032995</td>\n",
       "      <td>0.126540</td>\n",
       "      <td>1.053349</td>\n",
       "      <td>0.021622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>linear</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.941158</td>\n",
       "      <td>0.002491</td>\n",
       "      <td>0.939302</td>\n",
       "      <td>0.002534</td>\n",
       "      <td>9</td>\n",
       "      <td>4.705361</td>\n",
       "      <td>0.114248</td>\n",
       "      <td>1.077988</td>\n",
       "      <td>0.034324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>linear</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.941158</td>\n",
       "      <td>0.002491</td>\n",
       "      <td>0.939302</td>\n",
       "      <td>0.002534</td>\n",
       "      <td>9</td>\n",
       "      <td>4.711354</td>\n",
       "      <td>0.076673</td>\n",
       "      <td>1.085313</td>\n",
       "      <td>0.016436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>linear</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.941158</td>\n",
       "      <td>0.002491</td>\n",
       "      <td>0.939302</td>\n",
       "      <td>0.002534</td>\n",
       "      <td>9</td>\n",
       "      <td>4.859334</td>\n",
       "      <td>0.218558</td>\n",
       "      <td>1.083661</td>\n",
       "      <td>0.042348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>linear</td>\n",
       "      <td>0.1</td>\n",
       "      <td>10</td>\n",
       "      <td>0.935251</td>\n",
       "      <td>0.002460</td>\n",
       "      <td>0.933240</td>\n",
       "      <td>0.002479</td>\n",
       "      <td>14</td>\n",
       "      <td>5.352372</td>\n",
       "      <td>0.132732</td>\n",
       "      <td>1.629639</td>\n",
       "      <td>0.036515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>linear</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.935251</td>\n",
       "      <td>0.002460</td>\n",
       "      <td>0.933240</td>\n",
       "      <td>0.002479</td>\n",
       "      <td>14</td>\n",
       "      <td>5.408309</td>\n",
       "      <td>0.158492</td>\n",
       "      <td>1.602026</td>\n",
       "      <td>0.031360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>linear</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.935251</td>\n",
       "      <td>0.002460</td>\n",
       "      <td>0.933240</td>\n",
       "      <td>0.002479</td>\n",
       "      <td>14</td>\n",
       "      <td>5.381361</td>\n",
       "      <td>0.314528</td>\n",
       "      <td>1.583318</td>\n",
       "      <td>0.017307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>linear</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.935251</td>\n",
       "      <td>0.002460</td>\n",
       "      <td>0.933240</td>\n",
       "      <td>0.002479</td>\n",
       "      <td>14</td>\n",
       "      <td>5.379697</td>\n",
       "      <td>0.290750</td>\n",
       "      <td>1.618967</td>\n",
       "      <td>0.030714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>linear</td>\n",
       "      <td>0.001</td>\n",
       "      <td>10</td>\n",
       "      <td>0.820560</td>\n",
       "      <td>0.003061</td>\n",
       "      <td>0.820716</td>\n",
       "      <td>0.002523</td>\n",
       "      <td>23</td>\n",
       "      <td>13.531663</td>\n",
       "      <td>0.669259</td>\n",
       "      <td>5.030691</td>\n",
       "      <td>0.034669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>linear</td>\n",
       "      <td>0.001</td>\n",
       "      <td>1</td>\n",
       "      <td>0.820560</td>\n",
       "      <td>0.003061</td>\n",
       "      <td>0.820716</td>\n",
       "      <td>0.002523</td>\n",
       "      <td>23</td>\n",
       "      <td>13.771021</td>\n",
       "      <td>0.215438</td>\n",
       "      <td>5.004022</td>\n",
       "      <td>0.023020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>linear</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.820560</td>\n",
       "      <td>0.003061</td>\n",
       "      <td>0.820716</td>\n",
       "      <td>0.002523</td>\n",
       "      <td>23</td>\n",
       "      <td>13.694333</td>\n",
       "      <td>0.516697</td>\n",
       "      <td>5.133015</td>\n",
       "      <td>0.043570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>linear</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.820560</td>\n",
       "      <td>0.003061</td>\n",
       "      <td>0.820716</td>\n",
       "      <td>0.002523</td>\n",
       "      <td>23</td>\n",
       "      <td>14.695993</td>\n",
       "      <td>0.285069</td>\n",
       "      <td>5.127999</td>\n",
       "      <td>0.195992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>rbf</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0.875015</td>\n",
       "      <td>0.001166</td>\n",
       "      <td>0.854059</td>\n",
       "      <td>0.001270</td>\n",
       "      <td>20</td>\n",
       "      <td>36.756284</td>\n",
       "      <td>4.756048</td>\n",
       "      <td>7.552729</td>\n",
       "      <td>0.759239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>rbf</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0.956291</td>\n",
       "      <td>0.003099</td>\n",
       "      <td>0.954532</td>\n",
       "      <td>0.003094</td>\n",
       "      <td>3</td>\n",
       "      <td>14.426070</td>\n",
       "      <td>0.047478</td>\n",
       "      <td>1.713715</td>\n",
       "      <td>0.011309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>rbf</td>\n",
       "      <td>10</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.960643</td>\n",
       "      <td>0.003516</td>\n",
       "      <td>0.959190</td>\n",
       "      <td>0.003562</td>\n",
       "      <td>1</td>\n",
       "      <td>5.120979</td>\n",
       "      <td>0.193190</td>\n",
       "      <td>1.081000</td>\n",
       "      <td>0.026419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>rbf</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.922142</td>\n",
       "      <td>0.004125</td>\n",
       "      <td>0.919636</td>\n",
       "      <td>0.004102</td>\n",
       "      <td>19</td>\n",
       "      <td>9.855017</td>\n",
       "      <td>0.179574</td>\n",
       "      <td>3.431317</td>\n",
       "      <td>0.051771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>rbf</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0.869181</td>\n",
       "      <td>0.001667</td>\n",
       "      <td>0.846000</td>\n",
       "      <td>0.001813</td>\n",
       "      <td>22</td>\n",
       "      <td>35.637971</td>\n",
       "      <td>0.566107</td>\n",
       "      <td>7.049683</td>\n",
       "      <td>0.008972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>rbf</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.956909</td>\n",
       "      <td>0.002347</td>\n",
       "      <td>0.955050</td>\n",
       "      <td>0.002325</td>\n",
       "      <td>2</td>\n",
       "      <td>7.301669</td>\n",
       "      <td>0.070471</td>\n",
       "      <td>1.728997</td>\n",
       "      <td>0.050258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>rbf</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.953140</td>\n",
       "      <td>0.003435</td>\n",
       "      <td>0.951427</td>\n",
       "      <td>0.003439</td>\n",
       "      <td>4</td>\n",
       "      <td>5.577998</td>\n",
       "      <td>0.182649</td>\n",
       "      <td>1.763340</td>\n",
       "      <td>0.036054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>rbf</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.851268</td>\n",
       "      <td>0.003838</td>\n",
       "      <td>0.847701</td>\n",
       "      <td>0.003263</td>\n",
       "      <td>21</td>\n",
       "      <td>15.354030</td>\n",
       "      <td>0.174059</td>\n",
       "      <td>5.918664</td>\n",
       "      <td>0.053784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>rbf</td>\n",
       "      <td>0.1</td>\n",
       "      <td>10</td>\n",
       "      <td>0.744913</td>\n",
       "      <td>0.002013</td>\n",
       "      <td>0.645350</td>\n",
       "      <td>0.003602</td>\n",
       "      <td>27</td>\n",
       "      <td>20.254993</td>\n",
       "      <td>0.175297</td>\n",
       "      <td>7.919667</td>\n",
       "      <td>0.027978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>rbf</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.940859</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>0.937454</td>\n",
       "      <td>0.001535</td>\n",
       "      <td>13</td>\n",
       "      <td>8.284653</td>\n",
       "      <td>0.105175</td>\n",
       "      <td>2.822995</td>\n",
       "      <td>0.020064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>rbf</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.930231</td>\n",
       "      <td>0.003926</td>\n",
       "      <td>0.927621</td>\n",
       "      <td>0.003810</td>\n",
       "      <td>18</td>\n",
       "      <td>9.129331</td>\n",
       "      <td>0.062514</td>\n",
       "      <td>3.293683</td>\n",
       "      <td>0.043257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>rbf</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.242080</td>\n",
       "      <td>0.022139</td>\n",
       "      <td>0.541993</td>\n",
       "      <td>0.007222</td>\n",
       "      <td>29</td>\n",
       "      <td>19.812339</td>\n",
       "      <td>0.009870</td>\n",
       "      <td>8.098032</td>\n",
       "      <td>0.051829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>rbf</td>\n",
       "      <td>0.001</td>\n",
       "      <td>10</td>\n",
       "      <td>0.227869</td>\n",
       "      <td>0.322183</td>\n",
       "      <td>0.493642</td>\n",
       "      <td>0.017983</td>\n",
       "      <td>30</td>\n",
       "      <td>20.791334</td>\n",
       "      <td>0.341767</td>\n",
       "      <td>8.177011</td>\n",
       "      <td>0.064050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>rbf</td>\n",
       "      <td>0.001</td>\n",
       "      <td>1</td>\n",
       "      <td>0.470075</td>\n",
       "      <td>0.190514</td>\n",
       "      <td>0.599364</td>\n",
       "      <td>0.023434</td>\n",
       "      <td>28</td>\n",
       "      <td>20.826692</td>\n",
       "      <td>0.257830</td>\n",
       "      <td>8.139637</td>\n",
       "      <td>0.101500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rbf</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.227869</td>\n",
       "      <td>0.322183</td>\n",
       "      <td>0.493642</td>\n",
       "      <td>0.017983</td>\n",
       "      <td>30</td>\n",
       "      <td>20.201328</td>\n",
       "      <td>0.267141</td>\n",
       "      <td>8.270029</td>\n",
       "      <td>0.096243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rbf</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.227869</td>\n",
       "      <td>0.322183</td>\n",
       "      <td>0.493642</td>\n",
       "      <td>0.017983</td>\n",
       "      <td>30</td>\n",
       "      <td>20.305662</td>\n",
       "      <td>0.775104</td>\n",
       "      <td>8.451359</td>\n",
       "      <td>0.267509</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   param_kernel param_C param_gamma  mean_test_F1  std_test_F1  \\\n",
       "30       linear      10          10      0.943794     0.001819   \n",
       "28       linear      10           1      0.943794     0.001819   \n",
       "26       linear      10         0.1      0.943794     0.001819   \n",
       "24       linear      10       0.001      0.943794     0.001819   \n",
       "22       linear       1          10      0.941158     0.002491   \n",
       "20       linear       1           1      0.941158     0.002491   \n",
       "18       linear       1         0.1      0.941158     0.002491   \n",
       "16       linear       1       0.001      0.941158     0.002491   \n",
       "14       linear     0.1          10      0.935251     0.002460   \n",
       "12       linear     0.1           1      0.935251     0.002460   \n",
       "10       linear     0.1         0.1      0.935251     0.002460   \n",
       "8        linear     0.1       0.001      0.935251     0.002460   \n",
       "6        linear   0.001          10      0.820560     0.003061   \n",
       "4        linear   0.001           1      0.820560     0.003061   \n",
       "2        linear   0.001         0.1      0.820560     0.003061   \n",
       "0        linear   0.001       0.001      0.820560     0.003061   \n",
       "31          rbf      10          10      0.875015     0.001166   \n",
       "29          rbf      10           1      0.956291     0.003099   \n",
       "27          rbf      10         0.1      0.960643     0.003516   \n",
       "25          rbf      10       0.001      0.922142     0.004125   \n",
       "23          rbf       1          10      0.869181     0.001667   \n",
       "21          rbf       1           1      0.956909     0.002347   \n",
       "19          rbf       1         0.1      0.953140     0.003435   \n",
       "17          rbf       1       0.001      0.851268     0.003838   \n",
       "15          rbf     0.1          10      0.744913     0.002013   \n",
       "13          rbf     0.1           1      0.940859     0.001600   \n",
       "11          rbf     0.1         0.1      0.930231     0.003926   \n",
       "9           rbf     0.1       0.001      0.242080     0.022139   \n",
       "7           rbf   0.001          10      0.227869     0.322183   \n",
       "5           rbf   0.001           1      0.470075     0.190514   \n",
       "3           rbf   0.001         0.1      0.227869     0.322183   \n",
       "1           rbf   0.001       0.001      0.227869     0.322183   \n",
       "\n",
       "    mean_test_Accuracy  std_test_Accuracy  rank_test_Accuracy  mean_fit_time  \\\n",
       "30            0.941964           0.001815                   5      13.844085   \n",
       "28            0.941964           0.001815                   5      12.021735   \n",
       "26            0.941964           0.001815                   5      11.294003   \n",
       "24            0.941964           0.001815                   5      11.250313   \n",
       "22            0.939302           0.002534                   9       5.032995   \n",
       "20            0.939302           0.002534                   9       4.705361   \n",
       "18            0.939302           0.002534                   9       4.711354   \n",
       "16            0.939302           0.002534                   9       4.859334   \n",
       "14            0.933240           0.002479                  14       5.352372   \n",
       "12            0.933240           0.002479                  14       5.408309   \n",
       "10            0.933240           0.002479                  14       5.381361   \n",
       "8             0.933240           0.002479                  14       5.379697   \n",
       "6             0.820716           0.002523                  23      13.531663   \n",
       "4             0.820716           0.002523                  23      13.771021   \n",
       "2             0.820716           0.002523                  23      13.694333   \n",
       "0             0.820716           0.002523                  23      14.695993   \n",
       "31            0.854059           0.001270                  20      36.756284   \n",
       "29            0.954532           0.003094                   3      14.426070   \n",
       "27            0.959190           0.003562                   1       5.120979   \n",
       "25            0.919636           0.004102                  19       9.855017   \n",
       "23            0.846000           0.001813                  22      35.637971   \n",
       "21            0.955050           0.002325                   2       7.301669   \n",
       "19            0.951427           0.003439                   4       5.577998   \n",
       "17            0.847701           0.003263                  21      15.354030   \n",
       "15            0.645350           0.003602                  27      20.254993   \n",
       "13            0.937454           0.001535                  13       8.284653   \n",
       "11            0.927621           0.003810                  18       9.129331   \n",
       "9             0.541993           0.007222                  29      19.812339   \n",
       "7             0.493642           0.017983                  30      20.791334   \n",
       "5             0.599364           0.023434                  28      20.826692   \n",
       "3             0.493642           0.017983                  30      20.201328   \n",
       "1             0.493642           0.017983                  30      20.305662   \n",
       "\n",
       "    std_fit_time  mean_score_time  std_score_time  \n",
       "30      1.157593         1.052269        0.090952  \n",
       "28      1.461690         0.967000        0.114822  \n",
       "26      0.528975         0.904012        0.009436  \n",
       "24      1.156103         0.953658        0.102751  \n",
       "22      0.126540         1.053349        0.021622  \n",
       "20      0.114248         1.077988        0.034324  \n",
       "18      0.076673         1.085313        0.016436  \n",
       "16      0.218558         1.083661        0.042348  \n",
       "14      0.132732         1.629639        0.036515  \n",
       "12      0.158492         1.602026        0.031360  \n",
       "10      0.314528         1.583318        0.017307  \n",
       "8       0.290750         1.618967        0.030714  \n",
       "6       0.669259         5.030691        0.034669  \n",
       "4       0.215438         5.004022        0.023020  \n",
       "2       0.516697         5.133015        0.043570  \n",
       "0       0.285069         5.127999        0.195992  \n",
       "31      4.756048         7.552729        0.759239  \n",
       "29      0.047478         1.713715        0.011309  \n",
       "27      0.193190         1.081000        0.026419  \n",
       "25      0.179574         3.431317        0.051771  \n",
       "23      0.566107         7.049683        0.008972  \n",
       "21      0.070471         1.728997        0.050258  \n",
       "19      0.182649         1.763340        0.036054  \n",
       "17      0.174059         5.918664        0.053784  \n",
       "15      0.175297         7.919667        0.027978  \n",
       "13      0.105175         2.822995        0.020064  \n",
       "11      0.062514         3.293683        0.043257  \n",
       "9       0.009870         8.098032        0.051829  \n",
       "7       0.341767         8.177011        0.064050  \n",
       "5       0.257830         8.139637        0.101500  \n",
       "3       0.267141         8.270029        0.096243  \n",
       "1       0.775104         8.451359        0.267509  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(grid.cv_results_)\n",
    "df1 = df[['param_kernel','param_C','param_gamma','mean_test_F1','std_test_F1','mean_test_Accuracy','std_test_Accuracy','rank_test_Accuracy','mean_fit_time','std_fit_time','mean_score_time','std_score_time']]\n",
    "df1 = df1.sort_values(['param_kernel','param_C','param_gamma'], ascending=[True, False, False])\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC(C=10, cache_size=6000, class_weight='balanced', coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "\n",
      "\n",
      "\n",
      "Score of the best hyperparametered model:   0.9636309875813128\n"
     ]
    }
   ],
   "source": [
    "# scale the data : réduire le execution time\n",
    "scaling = MinMaxScaler(feature_range=(-1,1)).fit(X_train)\n",
    "xtrain = scaling.transform(X_train)\n",
    "xtest = scaling.transform(X_test)\n",
    "\n",
    "# afficher le meilleur modèle et le stocker\n",
    "best_model = grid.best_estimator_\n",
    "best_model.fit(xtrain, Y_train)\n",
    "print(best_model)\n",
    "print(\"\\n\\n\")\n",
    "print(\"Score of the best hyperparametered model:   \" + str(best_model.score(xtest,Y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rapport"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1 - \n",
    "Parmi les méthodes de validation (Leave-one-out cross-validation, Leave-p-out crossvalidation, k-fold cross-validation), présentez l’approche de validation que vous avez utilisée et pourquoi vous l’avez utilisée. Faites des liens avec les modèles d’apprentissage à l’étude. Rappelez-vous que vous pouvez sélectionner plus qu’une méthode qui servira à la validation des deux modèles à l’études dans ce laboratoire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2 - \n",
    "Décrivez la méthode de normalisation de donnés utilisée. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3 - \n",
    "Décrivez la structure et le choix de votre modèle d’apprentissage MLP. Faites un parallèle entre votre description et votre implémentation. Quelle est la fonction de coût utilisée ? Pourquoi avez-vous utilisé cette fonction plutôt qu’une autre (quels sont les avantages et inconvénients ?) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4 - \n",
    "Avec les graphiques créés par TensorBoard (TensorFlow) ou Visdom (PyTorch), après combien d’itérations/epochs êtes-vous en état de surapprentissage [overfitting]? À votre avis, quel est le nombre d’epochs optimal pour votre modèle?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5 - \n",
    "Pour chacun des modèles d’apprentissage élaborés, présentez sous forme de tableau et de graphique les résultats de la précision (accuracy) et du score F1. Expliquez l’impact des hyperparamètres sur les performances du modèle MLP. Comment se traduisent les divers changements de paramètres ou pourquoi ont-il cet impact sur la performance du modèle ? Prenez en note le temps d’exécution de vos modèles autant en phase d’apprentissage qu’en phase de test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6 - \n",
    "Présentez brièvement la méthode que vous avez utilisée afin de trouver le meilleur modèle SVM. Quels ont été vos résultats? Quels sont les impacts des hyperparamètres et leur utilité respective?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7 - \n",
    "Quel est l’impact de la taille de l’ensemble d’apprentissages sur la performance de classification des différents modèles?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 8 - \n",
    "Quel type de classificateur recommanderiez-vous pour l’ensemble de données des\n",
    "galaxies et dans quelles conditions (par exemple mais non exhaustif, le nombre de données privilégié, les hyperparamètres, le temps de calcul, le matériel nécessaire, les scores de performance)? Discutez des performances que vous avez obtenues entre les modèles d’apprentissage utilisant les réseaux de neurones et les modèles SVM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 9 - \n",
    "Formulez quelques pistes d’amélioration des classificateurs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
